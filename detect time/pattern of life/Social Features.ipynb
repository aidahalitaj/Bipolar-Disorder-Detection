{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.append('../../../Mental_Disorder/3_feature_visualization') # get old tweets library\n",
    "import age_gender_predictor\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLangRatio(cursor):\n",
    "    lang_ratios = {}\n",
    "    for tweet in cursor:\n",
    "        lang = 1 if tweet[\"lang\"] == \"en\" else 0\n",
    "        user_id = tweet[\"user\"][\"id\"]\n",
    "        if user_id in lang_ratios:\n",
    "            lang_ratios[user_id].append(lang)\n",
    "        else:\n",
    "            lang_ratios[user_id] = [lang]\n",
    "    for user_id, ratio in lang_ratios.items():\n",
    "        lang_ratios[user_id] = np.sum(ratio) / len(ratio)\n",
    "    return lang_ratios\n",
    "\n",
    "def getUsersTweets(dbName,collectionName, en_threshold=0.9):\n",
    "    cursor = MongoClient(\"localhost\", 27017)[dbName][collectionName].find()\n",
    "    lang_ratios = getLangRatio(cursor)\n",
    "\n",
    "    cursor = MongoClient(\"localhost\", 27017)[dbName][collectionName].find()\n",
    "    usersTweets = {}\n",
    "    for tweet in cursor:\n",
    "        userID = tweet[\"user\"][\"id\"]\n",
    "        if lang_ratios[userID] < en_threshold:\n",
    "            continue\n",
    "        #Processing emotions from Carlos' API\n",
    "        emotion =  tweet[\"emotion\"][\"groups\"][0][\"name\"]\n",
    "        if len(tweet[\"emotion\"][\"groups\"]) > 1:\n",
    "            emotion_2 = tweet[\"emotion\"][\"groups\"][1][\"name\"]\n",
    "            \n",
    "        ambiguous = True if tweet['emotion']['ambiguous'] == 'yes' else False\n",
    "       \n",
    "        if len(tweet[\"emotion\"][\"groups\"]) > 1:\n",
    "            emotion_2 = tweet[\"emotion\"][\"groups\"][1][\"name\"]    \n",
    "        else:\n",
    "            emotion_2 = None\n",
    "        if tweet[\"polarity\"] == \"positive\":\n",
    "            polarity = 1\n",
    "        elif tweet[\"polarity\"] == \"negative\":\n",
    "            polarity = -1\n",
    "        else:\n",
    "            polarity = 0\n",
    "   \n",
    "            \n",
    "        date = tweet[\"created_at\"]\n",
    "        text = tweet['text']\n",
    "\n",
    "        if userID not in usersTweets:\n",
    "            usersTweets[userID] = {}\n",
    "        if date not in usersTweets[userID]:\n",
    "            usersTweets[userID][date] = {}\n",
    "            \n",
    "        usersTweets[userID][date]['text'] = text\n",
    "        usersTweets[userID][date]['polarity'] =  polarity\n",
    "        usersTweets[userID][date]['emotion'] =  emotion\n",
    "        usersTweets[userID][date]['emotion_2'] =  emotion_2\n",
    "        usersTweets[userID][date]['ambiguous'] =  ambiguous\n",
    "    return usersTweets\n",
    "\n",
    "def timeSeriesTransform(usersEmotions):\n",
    "    for userID in usersEmotions:\n",
    "        usersEmotions[userID] = pd.DataFrame.from_dict(usersEmotions[userID], orient='index').fillna(0)\n",
    "        usersEmotions[userID]['dt'] = np.zeros(usersEmotions[userID].shape[0],dtype=float)\n",
    "        usersEmotions[userID].loc[:-1,'dt'] = (usersEmotions[userID].index[1:].values - usersEmotions[userID].index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return list(usersEmotions.values())\n",
    "\n",
    "def getHTTPRows(timeSeries):\n",
    "    count = 0\n",
    "    patterns = ['http://','https://']\n",
    "    conditions = timeSeries['text'].str.contains(patterns[0])\n",
    "    for pattern in patterns[1:]:\n",
    "        conditions = conditions | timeSeries['text'].str.contains(pattern)\n",
    "\n",
    "    return conditions.values\n",
    "\n",
    "def userFilter(group, spam_threshold=0.5,tweets_threshold=100):    #Spam and inactive user filter\n",
    "    new_group = []\n",
    "    for timeSeries in group:\n",
    "        http_rows = getHTTPRows(timeSeries)\n",
    "        average_http_count = np.sum(http_rows) / timeSeries.shape[0]\n",
    "        if (average_http_count < spam_threshold) and (timeSeries.shape[0] > tweets_threshold):\n",
    "            new_group.append(timeSeries)\n",
    "    return new_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regular_tweets =  getUsersTweets(\"eric\",\"regularUser_en_fixed_emotion\")\n",
    "regular_timeSeries = timeSeriesTransform(regular_tweets)\n",
    "regular_clean = userFilter(regular_timeSeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipolar User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTweets():\n",
    "    # {username:{int(date):{[(datetime,content,sentiment),...]}}}\n",
    "    tweets_dict = defaultdict(lambda: defaultdict(lambda:[]))\n",
    "    with open('../organized/date_sentiment_tweets') as tweets:\n",
    "        for line in tweets.readlines():\n",
    "            username, date, datetime, content, sentiment = line.split('\\t')\n",
    "            tweets_dict[username][int(date)].append((datetime, content, sentiment))\n",
    "\n",
    "    return tweets_dict\n",
    "\n",
    "def TweetsFormating(tweets_dict, en_threshold=0.9):\n",
    "    usersTweets = {}\n",
    "    for user in tweets_dict:\n",
    "        userID = user\n",
    "        for date in tweets_dict[user]:\n",
    "            for tweet_info in tweets_dict[user][date]:\n",
    "                date, content, polarity = tweet_info\n",
    "   \n",
    "            \n",
    "#         date = tweet[\"created_at\"]\n",
    "                date = datetime.strptime(str(date), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                text = content\n",
    "\n",
    "                if userID not in usersTweets:\n",
    "                    usersTweets[userID] = {}\n",
    "                if date not in usersTweets[userID]:\n",
    "                    usersTweets[userID][date] = {}\n",
    "\n",
    "                usersTweets[userID][date]['text'] = text\n",
    "                usersTweets[userID][date]['polarity'] =  int(polarity.strip())\n",
    "                usersTweets[userID][date]['emotion'] =  None\n",
    "                usersTweets[userID][date]['emotion_2'] =  None\n",
    "                usersTweets[userID][date]['ambiguous'] =  True\n",
    "    return usersTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# {username:{int(date):{[(datetime,content,sentiment),...]}}}\n",
    "bd_tweets_dict = loadTweets()\n",
    "bd_tweets = TweetsFormating(bd_tweets_dict)\n",
    "bd_timeSeries = timeSeriesTransform(bd_tweets)\n",
    "bd_clean = userFilter(bd_timeSeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Tweet Frequence (All & Late)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTweetRate(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    delta_time = np.max(timeSeries.index.values) - np.min(timeSeries.index.values)\n",
    "    totla_duration = (delta_time).astype('timedelta64[h]') / np.timedelta64(24, 'h')\n",
    "    return total_tweets / float(totla_duration)\n",
    "\n",
    "\n",
    "\n",
    "def thirdPronuonDetect(words, matcher=re.compile(\"@[a-z]+\")):\n",
    "    for word in words:\n",
    "        if word == \"@\":\n",
    "            continue\n",
    "        elif matcher.search(word):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def seriesContains(timeSeries):\n",
    "    match_function = np.vectorize(thirdPronuonDetect)\n",
    "    return match_function(timeSeries[\"text\"].str.lower().str.split().values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mention Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMentioRate(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    total_mentions = np.sum(seriesContains(timeSeries))\n",
    "    return total_mentions / float(total_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFrequentMentions(timeSeries, lowerbound = 3):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    friends_mentions = {}\n",
    "    texts = timeSeries[\"text\"].values\n",
    "    for text in texts:\n",
    "        terms = text.strip().split()\n",
    "        for word in terms:\n",
    "            if word[0] == '@' and len(word) > 1:\n",
    "                friends_mentions[word] = friends_mentions.get(word, 0) +1\n",
    "    frequent_frients = [screen_name for screen_name, mentions in friends_mentions.items() if mentions >= lowerbound]\n",
    "    return len(frequent_frients)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def getUniqueMentions(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    friends_set = set()\n",
    "    texts = timeSeries[\"text\"].values\n",
    "    for text in texts:\n",
    "        terms = text.strip().split()\n",
    "        for word in terms:\n",
    "            if word[0] == '@' and len(word) > 1:\n",
    "                friends_set.add(word)\n",
    "    return len(friends_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSocialFeature_group(group):\n",
    "    social_features = {\"tweets_rate\": [],\"mention_rate\": [],\"unique_mentions\": [],\"frequent_mentions\": []}\n",
    "    for timeSeries in group:\n",
    "        social_features[\"tweets_rate\"].append(getTweetRate(timeSeries))\n",
    "        social_features[\"mention_rate\"].append(getMentioRate(timeSeries))\n",
    "        social_features[\"unique_mentions\"].append(getUniqueMentions(timeSeries))\n",
    "        social_features[\"frequent_mentions\"].append(getFrequentMentions(timeSeries))\n",
    "    return social_features\n",
    "\n",
    "def summaryTable(groups,names, method, style=\"default\", tablefmt = \"simple\"):\n",
    "    header = [\"category\"]\n",
    "    group_counts = []\n",
    "    base = method(groups[0])\n",
    "    base_labels = [0] * len(groups[0])\n",
    "    contents = []\n",
    "    for name in names:\n",
    "        header.append(name + \" C\")\n",
    "        header.append(name + \" P\")\n",
    "        \n",
    "        \n",
    "    for group in groups:\n",
    "        group_counts.append(method(group))\n",
    "        \n",
    "        \n",
    "        \n",
    "    if style == \"default\":\n",
    "        for category, base_count in base.items():\n",
    "            content = [category]\n",
    "            for g,group in enumerate(groups):\n",
    "\n",
    "                labels = base_labels + ([1]*len(group))\n",
    "                counts = base_count + group_counts[g][category]\n",
    "                c, p = spearmanr(labels, counts)\n",
    "\n",
    "                content.append(c)\n",
    "                content.append(p)\n",
    "\n",
    "            contents.append(content)\n",
    "  \n",
    "        contents = sorted(contents, key=lambda pair: abs(pair[1]), reverse=True)           \n",
    "#        \n",
    "        print(tabulate(contents, headers=header,floatfmt=\".2f\", tablefmt=tablefmt))\n",
    "    \n",
    "    return contents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = [ regular_clean, bd_clean]\n",
    "group_names = [\"Regular\", \"Bipolar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======Social Features======\n",
      "\n",
      "\n",
      "======Mean Distribution======\n",
      "\n",
      "           tweets_rate    mention_rate    unique_mentions    frequent_mentions\n",
      "-------  -------------  --------------  -----------------  -------------------\n",
      "Regular        3.39794        0.428638            171.801                   14\n",
      "Bipolar       13.4388         0.410792           1249.87                   248\n",
      "\n",
      "======Correlation======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "header = [\"tweets_rate\", \"mention_rate\", \"unique_mentions\", \"frequent_mentions\"]\n",
    "contents = []\n",
    "for i, group in enumerate(groups):\n",
    "   \n",
    "    group_name = group_names[i]\n",
    "    content = [group_name]\n",
    "    tweets_rate = np.mean([getTweetRate(timeSeries) for timeSeries in group])\n",
    "    mention_rate = np.mean([getMentioRate(timeSeries) for timeSeries in group])\n",
    "    unique_mentions = np.mean([getUniqueMentions(timeSeries) for timeSeries in group])\n",
    "    frequent_mention = np.mean([getFrequentMentions(timeSeries) for timSeries in group])    \n",
    "    content += [tweets_rate, mention_rate, unique_mentions, frequent_mention]\n",
    "    contents.append(content)\n",
    "print(\"\\n======Social Features======\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n======Mean Distribution======\\n\")\n",
    "\n",
    "print(tabulate(contents, headers=header,))\n",
    "print(\"\\n======Correlation======\\n\")\n",
    "summaryTable(groups,group_names,getSocialFeature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
